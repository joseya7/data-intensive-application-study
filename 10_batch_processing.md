# 10. 일괄 처리(Batch processing)
* 세 가지 시스템
    * Services(Online systems)
        * 서비스는 클라이언트로부터 요청이나 지시가 올 때까지 기다린다.
        * 요청 하나가 들어오면 서비스는 가능한 빨리 요청을 처리해서 응답을 되돌려 보내야 한다.
        * **응답 시간**(Response time), **가용성**(Availability) 서비스 성능을 측정할 때 중요한 지표.
    * Batch processing systems (Offline systems)
        * 일괄 처리(Batch processing)은 매우 큰 입력 데이터를 받아 처리하는 작업을 수행.
        * 수 분, 수 일이 걸리기 때문에 사용자가 작업이 끝날 때까지 대기하지 않는다.
        * 주요 성능 지표로는 **처리량**(Throughput)이 대표적.
    * Stream processing systems (near-real-time systems)
        * 스트림 처리(Stream processing)는 일괄 처리 시스템과 마찬가지로 요청에 대해 응답하지 않음.
        * 일괄 처리 작업이 정해진 크기의 입력 데이터를 대상으로 작동한다면, 스트림처리는 입력 이벤트가 발생한 직후 바로 작동.
        * 이런 차이 때문에 스트림 처리 시스템은 같은 작업을 하는 일괄 처리시스템보다 지연 시간이 낮음.

* 맵리듀스와 일괄 처리 시스템
    * *Batch Processing*은 신뢰할 수 있고(**Reliable**), 확장 가능하며(**Scalable**), 유지 보수하기 쉬운(**maintainable**) 애플리케이션을 구축하는데 매우 중요한 구성요소.
    * *Batch Processing Algorithm*인 *MapReduce*는 "구글을 대규모로 확장 가능하게 만든 알고리즘"으로 불림.
    * 현재는 *MapReduce*의 중요성이 떨어지고 있지만, 왜 일괄 처리가 유용한지 알려줌.
    * 이번 장에서는, *MapReduce*를 알아보고, *MapReduce*와 다른 *Batch Processing Algorithm*과 *Framework*를 알아봄.

## 유닉스 도구로 일괄 처리하기 (Batch processing with Unix Tools)
* 먼저, 표준 유닉스가 추구하는 철학은 현대 Batch Processing Algorithm과 비슷한 부분이 있다. 유닉스 철학과 유닉스 도구가 주는 교훈을 통해, Batch Processing을 더 잘 이해할 수 있다.
* 예시를 통해, 표준 유닉스 도구를 사용해 데이터를 처리하는 방법을 살펴보자.
```
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel MAC OS X
10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115
Safari/537.36"
```
이 로그를 해석해보자면,
* 2015년 2월 27일 UTC 17시 55분 11초에 로그 생성
* 서버가 클라이언트 IP주소 `216.58.210.78`로부터 `css/typography.css`파일에 대한 요청을 받음.
* 비인증 사용자여서 `$remoute_user`가 하이픈으로 표시.
* 응답 상태는 `200`으로 요청은 성공, 응답크기는 `3.377바이트`
* 웹 브라우저는 Chrome, 파일이 `http://martin.kleppmann.com/`이라는 URL에서 참조.

### 단순 로그 분석
* 위와 같은 로그를 표준 유닉스 도구를 통해 "웹 사이트에서 가장 인기가 높은 페이지 5개"를 뽑는 다면
```
cat /var/log/nginx/access.log |   #1
    awk '{print $7}'   |          #2
    sort               |          #2
    uniq -c            |          #3
    sort -r -n         |          #4
    head -n 5          |          #6
```
* \#1. `cat /var/log/nginx/access.log` : 로그를 읽어들인다.
* \#2. `awk '{print $7}'` :`/css/typgraphy.css/` 를 출력
* \#3. `sort` : 요청 URL을 알파벳 순으로 정렬. 
* \#4. `uniq -c`:  중복 제거 및 중복 횟수를 함께 출력
* \#5. `sort -r -n` : 요청 URL을 요청 수 기준으로 다시 정렬.
* \#6. `head` : 맨 앞 5줄만 출력
* 위의 유닉스 쉘의 결과는 아래와 같이 나올 수 있다.
    ```
    4189 /favicon.ico
    3631 /2013/05/25/improving-security-of-ssh-private-keys.html
    2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
    1309 /
    915  /css/typography.css
    ```
#### 연쇄 명령 대 맞춤형 프로그램(chain of commands vs. custom program)
*  위의 유닉스 명령어들을 루비로 작성할 수 있다.
    ```
    counts = Hash.new(0)                                                #1                   

    File.open('/var/log/nginx/access.log') do |file|
        file.each do |line|
            url = line.split[6]                                         #2
            counts[url] += 1                                            #3
        end
    end

    top5 = coutns.map{|url, count| [count, url]}.sort.reserve[0...5]    #4
    top5.each{|count, url| puts "#{count} #{url}" }                     #5
    ```

* \#1. `counts = Hash.new(0)`  : 각 URL이 몇 번 나왔는지 저장할 해시 테이블이다. 
* \#2. `url = line.split[6] ` : 공백으로 분리, URL이 있는 6번째 인덱스 추출
* \#3. `counts[url] += 1 ` : URL의 카운트를 증가
* \#4. `coutns.map{|url, count| [count, url]}.sort.reserve[0...5]`: 해시 테이블을 내림차순 정렬
* \#5. `top5.each{|count, url| puts "#{count} #{url}" }  ` : 상위 5개 항목을 출력
* 위 두 (유닉스 vs 루비)의 결과는 차이가 없지만, 가장 큰 다른 차이점이 있다 : *실행 흐름*(execution flow). (대용량 파일에서 확연한 차이.)


#### 정렬 대 인메모리 집계
* 위 두 스크립트의 차이는 아래와 같다.
    * **Ruby** : URL 해시 테이블을 메모리에 유지. URL의 수를 맵핑
    * **Unix Tools** : 해시 테이블이 없음. 정렬된 목록에서 같은 URL이 반복해서 나타남.
* *Ruby*, *Unix* 둘 중 무엇이 좋을지는 **서로 다른** URL들이 얼마나 되느냐에 따라 다르다.
    * 백만 개의 로그 항목이 모두 같은 URL 하나만을 가리키면 해시테이블이 유용.
        * 한 개의 URL과 카운트 값을 저장할 만큼만 필요하기 때문.
        * 작업 세트가 작다면 인메모리 해시 테이블도 잘 작동.
* 반면, **허용 메모리**보다 **작업세트가 크다면** 정렬 접근법(*Unix tools*)을 사용하는것이 좋음.
![image](https://user-images.githubusercontent.com/30207544/183413964-6d95486b-59c8-46f9-9ea7-1014f510edf1.png)
    * 이런 정렬 접근법은 디스크를 효율적으로 사용.
        * 기본적으로, SS테이블, LSM트리의 원리와 매우 비슷(p.76)
            1. 데이터 Chunk를 메모리에서 정렬
            2. Chunk를 Segment 파일로 디스크에 저장.
            3. 정렬된 Segment 파일 여러개를 한 개의 큰 정렬 파일로 병합.
        * 이 패턴은 디스크 상에서 좋은 성능을 낼 수 있음.
    * GNU Coreutils(리눅스)에 포함된 `sort` 유틸리티가 위와 비슷.
        * 메모리보다 큰 데이터셋을 자동으로 디스크로 보냄.
        * 여러 CPU코어에서 자동으로 병렬로 정렬.
        * 이를 통해 Unix Pipe 명령어들이 메모리 부족 없이 손 쉽게 큰 데이터셋으로 확장 가능.


### 유닉스 철학
* 연쇄 명령(**Unix chain command**)을 사용해 쉽게 로그 파일을 분석할 수 있는 것은 유닉스 철학과 관련이 있음.
*  1978년에 기술된 유닉스 철학은 아래와 같다.
    1. 각 프로그램은 **한 가지 일만 하도록 작성하라**.
    2. 모든 프로그램의 출력은 아직 알려지지 않은 **다른 프로그램의 입력으로 쓰일 수 있다**고 생각하라.
    3. 소프트웨어를 빠르게 써볼 수 있게 설계하고 구축하라.
    4. 프로그래밍 작업을 줄이려면 미숙한 도움보단 도구를 사용하라.
* `bash` 같은 유닉스 셸을 사용하면 한 가지만 잘하는 작은 프로그램들(`sort`, `uniq`)을 조합하여 강력한 데이터 처리 작업을 쉽게 **구성**할 수 있다.
    * 이런 프로그램 중 다수는 서로 다른 그룹의 사람들이 만들었지만, 유연한 방식으로 함께 **조합**할 수 있다. 유닉스에 이런 결합성을 가능하게 한 것은 **동일 인터페이스**이다.
#### 동일 인터페이스(A uniform interface)
* 특정 프로그램이 다른 **어떤** 프로그램과도 연결 가능하려면 프로그램 **모두**가 같은 입출력 인터페이스를 사용해야 한다.
* 유닉스의 동일 인터페이스(Uniform interface)는 `File`이다.
* `File`은 정렬된 바이트의 연속이여서 매우 단순하다.
    * 이 같은 특성때문에, 같은 인터페이스로 `Filesystem`, 통신채널(`Unix Socket`, `stdin`, `stdout`), 장치 드라이버(`/dev/audio/`), `TCP connection` 등 여러 가지 것을 표현할 수 있다.
* 완벽하지 않음에도 유닉스의 동일 인터페이스(`Uniform interface`)는 여전히 *대단하다*. 
    * 예를 들어, 이메일 내용을 파이프로 연결해서, 데이터를 파싱하여 스프레드 시트에 넣고, 결과를 웹에 올리는 것은 쉽게 할 수 없다.

    * 오늘날 유닉스 도구처럼 **매끄럽게 협동**하는 프로그램이 있다는 것은 정상이 아니라 예외적이다.

#### 로직과 연결의 분리(Separtion of logic and wiring)
`cat select_user_info.sql | ./access_db.sh main > 'user_info.tsv'`
* 유닉스 도구는 표준 입력(`stdin`)과 표준 출력(`stdout`)을 사용한다.
    * 프로그램을 실행하고 아무것도 설정하지 않는다면 `stdin`은 키보드로부터 들어오고 `stdout`은 화면으로 출력한다.
    * 유닉스는 파이프를 통해 한 프로세스의 `stdout`을 다른 프로세스의 `stdin`과 연결한다.
    * 이처럼, 프로그램에서 입출력을 연결하는 부분을 분리하면 작은 도구로부터 큰 시스템을 구성하기가 훨씬 수월해진다.
#### 투명성과 실험(Transparency and Experimentation)
* 유닉스가 성공적인 이유는 진행사항을 파악하기가 쉽기 때문이다.
    1. 유닉스 명령에 들어가는 입력 파일은 **불변**(immutable)로 처리된다.
    2. 어느 시점이든 파이프라인을 중단하고, `less`를 통해 원하는 형태의 출력이 나오는지 확인할 수 있다.
    3. 특정 파이프라인 단계의 출력을 파일에 쓰고, 그 파일을 다음 단계의 입력으로 사용할 수 있다.
* 이러한 이유 때문에 유닉스 도구는 실험(Experimentation)을 할 때 자주 사용될 수 있다.

## 맵리듀스와 분산 파일시스템
* MapReduce는 Unix도구와 비슷한 면이 있지만, 수천 대의 장비로 분산해서 실행이 가능하다는 점에서 차이가 있다.
* MapReduce와 Unix는 아래와 같은 유사한 면을 가지고 있다.
    * 단일 MapReduce작업은 하나 이상의 입력을 받아, 하나 이상의 출력을 만들어낸다는 점.
    * MapReduce작업은 입력을 수정하지 않기 때문에, 출력을 생산하는 것 외에 다른 부수효과가 없다는 점.
    * MapReduce작업은 분산 파일 시스템상의 파일을 입력과 출력으로 사용한다는 점. 
* Hadoop MapReduce구현에서 이 파일 시스템은 HDFS라고 한다.
* HDFS는 **비공유 원칙**을 기반으로 하여, 공유 디스크 방식과는 반대다.
    * 공유 디스크 방식은 맞춤형 하드웨어를 사용하거나, 특별한 인프라를 사용해야 하지만, 비공유 방식은 일반적인 데이터센터 네트워크에 연결된 컴퓨터면 충분하다.
* HDFS는 매우 큰 하나의 파일시스템이되고, 데몬 프로세스를 통해, HDFS내의 실행 중인 모든 장비의 디스크를 사용할 수 있다.
![image](https://user-images.githubusercontent.com/30207544/183413405-2250da6c-3ff0-4752-a871-354c68706658.png)

    * **데몬 프로세스**는 다른 노드가 해당 장비에 저장된 파일에 접근 가능하게끔 네트워크 서비스를 제공.
    * **NameNode**라고 부르는 중앙 서버는 특정 특정 파일 블록이 어떤 저장됐는지 추적.
* HDFS에서 장비가 죽거나 디스크가 실패하는 경우에 대비하기 위해 파일 블록은 여러 장비에 복제.
* HDFS는 확장성이 뛰어나, 수만 대의 장비를 묶어 실행할 수 있고 수백 페타바이트에 달하는 용량을 가질 수도 있다.
* HDFS를 이용한 데이터 저장과 접근 비용은 범용 하드웨어와 오픈소스 소프트웨어를 사용하기 때문에 동급 용량의 전용 저장소를 사용하는 비용보다 훨씬 저렴하다.
### 맵리듀스 작업 실행하기
* 맵리듀스는 HDFS와 같은 분산 파일 시스템 위에서 대용량 데이터셋을 처리하는 코드를 작성하는 프로그래밍 프레임워크
* 맵리듀스의 데이터 처리패턴은 위의 로그 분석예제와 매우 비슷.
    ```
    로그 분석 예제                               맵리듀스
    cat /var/log/nginx/access.log |   <->   #1. 입력 파일을 읽어, Record로 쪼갠다(\n).
    awk '{print $7}'   |              <->   #2. Record마다 Mapper 함수 호출. 매핑(key:url, value:  )
    sort               |              <->   #3. key를 기준으로 정렬       
    uniq -c            |              <->   #4. key-value에 Reduce 함수 호출. 인접한 레코드의 수를 센다.
    sort -r -n         |              <->   #5.
    head -n 5          |         
    ```
    * 맵리듀스 작업은 4가지 단계로 수행.
        * *1단계*는 파일을 나누어 레코드를 만듬.
        * *3단계*는 정렬단계로 맵리듀스에 내재하된 단계라서 작성할 필요가 없음.
        * *2단계*(Map), *4단계*(Reduce)는 사용자가 직접 작성한 데이터 처리 코드.
* 맵리듀스 작업을 생성하려면 Mapper와 Reducer라는 두 가지 콜백함수를 구현해야한다.
![image](https://user-images.githubusercontent.com/30207544/183419940-57efee4c-5420-4e44-8b4c-f94fe0c6ffeb.png)
    * **Mapper**
        * Mapper는 모든 입력 레코드마다 한 번씩만 호출.
        * Mapper는 Key, Value값을 추출하는하는 작업.
        * Mapper는 정렬에 적합한 형태로 데이터를 준비하는 역할.
    * **Reducer**
        * Reducer는 Mapper가 생성한 Key-Value쌍을 받아 같은 키를 가진 레코드를 모음.
        * Reducer함수는 해당 값의 집합만큼 반복. 
        * Reducer의 예로, 동일한 URL이 출현한 횟수.
        * Reducer는 정렬된 데이터를 가공하는 역할.
* 웹 서버 로그 예제에서, 5번째 단계를 보면 두 번째 정렬 명령어가 있는데, 맵리듀스에서 두 번째 정렬이 필요하다면, 첫 번째 작업의 출력을 두 번째 작업의 입력으로 사용할 수 있다.
#### 맵리듀스의 분산 실행
![image](https://user-images.githubusercontent.com/30207544/183423220-18e997e0-7686-488b-9990-31f0b8d63f9f.png)
* Unix 명령어 파이프라인(command pipeline)과 맵리듀스의 가장 큰 차이점은, 맵리듀스가 병렬로 수행하는 코드를 직접 작성하지 않고도 여러 장비에서 동시에 처리가 가능하다는 점.
* 맵리듀스의 병렬 실행은 파티셔닝을 기반.
* 입력으로 HDFS상의 디렉터리를 사용. 각 입력 파일은 보통 크기가 수백 MB.
* MapTask 코드는 맵리듀스 프레임워크를 통해 장비에 복사. 장비는 입력 파일을 읽기 시작하여 한 번에 레코드 하나씩 읽어 Mapper 콜백함수로 전달.
* Reducer는 전달받은 키의 해시값을 사용하여, 같은 Key를 가진 Key-value Pair들을 같은 Reducer에서 처리하게 된다.
* Reducer를 기준으로 파티셔닝하고 정렬한 뒤 Mapper로부터 데이터 파티션을 복사하는 과정을 **Shuffle**이라고 한다.
* Reduce Task는 Mapper로부터 받은 데이터를 정렬된 순서로 병합한다.
* Reducer는 Key와 Iterator를 인자로 호출하고, 이 Iterator로 전달된 Key와 동일한 Key를 가진 레코드를 모두 훑을 수 있게 됨.
* 출력 레코드는 분산 파일 시스템에 파일로 기록된다.

#### 맵리듀스 워크플로
* 맵리듀스는 Task를 연결해 워크플로(Workflow)를 구성.
    * Task하나의 출력을 다른 MapReduce Task의 입력으로 사용하는 방식.
    * 연결된 MapReduce Task는 유닉스 명령 파이프라인(소량의 메모리 버퍼를 사용해 한 프로세스의 출력이 다른 프로세스의 입력으로 직접 전달되는 방식)과는 조금 다르다.
    * MapReduce Workflow는 각 명령의 출력을 임시 파일에 쓰고, 그 임시 파일로부터 입력을 읽는 방식이다.
* Workflow상에서 해당 Task의 입력 디렉터리를 생성하는 선행작업이 끝나야먄 다음 Task가 시작될 수 있다.
    * Airflow, Oozie, Azkaban, Luigi등이 MapReduce Workflow의 의존성을 관리하는 스케줄러 도구가 될 수 있다.
    * 큰 조직에서는,  50~100개의 Workflow를 사용하는 것이 일반적. 이런 복잡한 데이터 Flow를 관리하기 위해서는 위와 같은 스케줄러 도구가 필요.

### 리듀스 사이드 조인과 그룹화(Reduced Side joins and Grouping)
* 데이터베이스에서 적은 수의 레코드만 찾는다면, 데이터베이스는 일반적으로 **INDEX**를 사용해 관심 있는 레코드의 위치를 빨리 찾는다.
* 맵리듀스에서는 파일 집합이 입력으로 주어졌을 때 입력 파일 전체를 읽는다. (**Full Table Scan**) 이 경우, **Index Scan** 보다 터무니 없는 비용이 든다.
* OLAP는 대량의 레코드를 대상으로 집계 연산을 하기때문에 이렇게 전체 연산을 스캔하는 것은 일반적.
#### 정렬 병합 조인
![image](https://user-images.githubusercontent.com/30207544/184630239-d7ba29d7-2640-4751-bc00-b22787dd008f.png)
* Mapper는 입력 레코드로부터 키와 값을 추출하는 것.
    * 한 Mapper는 활동 이벤트를 훑어 `{key: user_id, value: url}`
    * 한 Mapper는 사용자 테이블을 읽어 `{key: user_id, value: dob}`

![image](https://user-images.githubusercontent.com/30207544/184631161-b8cbc1fe-5ac9-48c4-b038-bb85f7d12721.png)
* Mapper의 출력을 파티셔닝해 Key-Value 쌍으로 정렬하여, 같은 사용자의 활동이벤트와 사용자 레코드가 Reducer의 입력으로 인접해서 들어간다.
* Reducer는 `user_id`당 한번씩 호출하여, url, dob의 쌍을 출력. 이렇게하여, 맵리듀스 작업들이 각 URL을 본 사람들의 연령 분포를 집계할 수 있게 됨.
* 이렇게 Mapper출력을 키로 정렬하여, Reducer가 조인의 양측의 정렬된 레코드 목록을 병합하는 형태를 **정렬 병합 조인(sort-merge join)** 이라고 한다.
#### 같은 곳으로 연관된 데이터 가져오기
* Mapper와 정렬 프로세스는 특정 `user_id`로 정렬 병합 조인을 할 때 필요한 모든 데이터를 한 곳으로 모은다.
* 그래서 `user_id`별로 Reducer를 한 번만 호출하여 `JOIN`시 처리량을 높게 유지하면서 메모리 부담을 줄일 수 있다.
#### 그룹화
* `JOIN` 외에도 "같은 곳으로 관련 데이터를 모으는" 일반적인 사용 유형은 `GROUP BY`.
    * 각 그룹의 레코드 수를 세기: `COUNT(*)`
    * 각 그룹의 값을 합산 : `SUM(user_id)`
    * 필드 내에서 상위 k개의 레코드 고르기
* Mapper가 Key-value를 생성할 때 `GROUP BY`할 대상을 키로 하면, 파티션과 정렬 프로세스가 같은 키를 가진 모든 레코드를 같은 리듀서로 모음. 그리고 Reducer는 모아져 있는 데이터로 집계연산(`SUM`, `COUNT`)을 한다.  (`JOIN`과 유사) 
#### 쏠림 다루기(Handling Skew)
* 키 하나에 너무 많은 데이터가 쏠려있다면(Skew) "같은 키를 가지는 모든 레코드를 같은 장소를 모으는" 패턴은 제대로 작동하지 않는다.
* 예를 들어, 소셜 네트워크에 인플루언서는 팔로워가 수백만명에 이르지만, 대다수의 사람들은 많아아 수백 명 정도이다.
* 이렇게 불균형한 활성 데이터베이스 레코드를 린치핀 객체(Linchpin object) 또는 핫 키(Hot key)라 한다.
* 조인 입력에 핫 키가 존재하는 경우에 이를 완화활 몇 가지 알고리즘이 있다.
    * *Pig* : `쏠린 조인(Skewed join)` 는 어떤 키가 핫 키인지 결정하기 위해 샘플링 작업을 수행. 실제 조인을 수행할 때 Mapper는 핫 키를 가진 레코드를 Reducer 중 임의로 선택하여, 여러 리듀서가 처리하도록 한다.
    * *Crunch* : `공유 조인(Shared Join)`은 샘플링 작업 대신 핫 키를 명시적으로 지정.
    * *Hive* : 핫 키를 메타데이터에 지정. 핫 키와 관련된 레코드를 나머지 키와 별도 파일에 저장. 이 테이블을 조인싴리 때는 맵 사이드 조인을 사용해 처리.


### 맵 사이드 조인(Map-side join)
* `Reduce-Side` 접근법의 장점은 입력 데이터에 대한 특정 가정이 필요없다는 점.
    * 입력 데이터의 속성과 구조가 무엇이든 매퍼는 데이터를 조인을 할 수 있다.
    * 하지만, 정렬 후 `Reducer`로 복사 한 뒤 `Reducer` 입력을 병합하는 모든 과정에 상당한 비용이 듬.
* `Map-Side` 접근법은 입력 데이터에 대해 특정 가정이 가능하다면 조인을 더 효율적으로 수행할 수 있다.
    * 이 접근법은, Reducer는 물론 정렬 작업도 없다.
    * 대신, Mapper가 할 작업은 분산 파일 시스템에서 단순히 입력 파일 블럭 하나를 읽어 다시 해당 분산 파일 시스템에 출력하는 것이 전부다.
#### 브로드캐스트 해시 조인
* 매우 큰 데이터셋(A)과 작은 데이터셋(B)를 조인하는 경우에 Map-Side Join의 `Broadcast Hash Join`을 적용해 볼 수 있다.
 ![image](https://user-images.githubusercontent.com/30207544/184630239-d7ba29d7-2640-4751-bc00-b22787dd008f.png)
 * 예를 들어, 사용자 데이터셋(B)가 메모리에 들어갈 정도로 충분히 작다고 가정한다면
    * Mapper가 시작할 때 분산 파일 시스템에서 사용자 데이터베이스(B)를 읽어서 In-memory Hash table에 넣는다.
    * Mapper는 사용자 활동 이벤트를 모두 스캔할 수 있고, 각 이벤트의 사용자 ID를 해시 테이블에서 간단하게 조회할 수 있게 된다.
    * Mapper Task를 여러 개 사용하여, 각 Map Task에 조인할 큰 입력 파일 블록 중 하나를 할당할 수도 있다.
    * Braodcast라는 단어는 큰 입력의 파티션 하나를 담당하는 각 Mapper는 작은 입력 전체를 읽는다는 것을 의미한다.
    * Pig에서는 복제 조인, 하이브에서는 맵 조인이라고 불린다.
#### 파티션 해시 조인
* Map-Side Join의 입력을 파티셔닝한다면 Hash Join 접근법을 각 파티션에 독립적으로 적용할 수 있다.
* `user_id`의 마지막 십진수를 기준으로 파티셔닝해 재배열할 수 있다. (조인할 데이터셋 양쪽 모두 10개의 파티셔닝이 생긴다.)
    * 예를 들어, 3번 Mapper가 `user_id`가 3으로 끝나는 모든 사용자를 해시 테이블에 올리고 ID가 3으로 끝나는 사용자 활동 이벤트 모두를 스캔하는 식이다.
* 조인할 레코드 모두가 같은 번호의 파티션에 위치하기 때문에, 각 Mapper는 각 입력 데이터 셋 중 파티션 한 개만 읽어도 된다.
* 이 방법은 각 Mapper의 해시 테이블에 적재해야 할 데이터의 양을 줄일 수 있다는 점이 장점이다.  
#### 맵 사이드 조인을 사용하는 맵리듀스 워크플로
* MapReduce Join의 출력을 하위 작업에서 입력으로 사용할 때, Map-Side Join을 사용할 지 Reduce-Side Join을 사용할지에 따라 그 출력 구조가 달라진다.
    * Reduce Side Join은 Join Key로 파티셔닝하고 정렬해서 출력한다.
    * Map Side Join은 입력과 동일한 방법으로 파티셔닝하고 정렬한다.
* 앞서, 살펴본 것처럼 Map-Side Join을 수행하기 위해서는 크기, 정렬, 입력 데이터의 파티셔닝 같은 제약사항이 따른다. 
    * Join 전략을 최적화할 때는 분산 파일 시스템 내 저장된 데이터셋의 물리적 레이아웃 파악이 중요하다.
    * 파티션 수가 몇 개인지, 데이터가 어떤 키를 기준으로 파티셔닝되고 정렬됐는지도 꼭 알아야 한다.
* Hadoop 생태계에서는 데이터셋 파티셔닝 관련 메타데이터를 관리하는데 HCatalog나 Hive 메타스토어를 사용한다.


### 일괄 처리 워크플로의 출력
* 지금까지 맵리듀스 작업의 워크플로를 구현하는 데 쓰이는 다양한 알고리즘을 이야기했다. 
* 앞으로는 Batch Processing의 이 모든 작업을 수행하는 이유가 무엇인지에 대해 알아본다.
#### 검색 색인 구축
* 맵리듀스는 구글에서 검색 엔진에 사용할 색인을 구축하기 위해 처음 사용됐었는데 그 당시 사용된 워크플로는 5~10개의 맵리듀스 작업으로 구현됐다. 
* 현재 구글이 색인을 구축하는 목적으로는 맵리듀스를 더이상 사용하지 않지만 검색 색인을 구축하는 과정을 자세히 살펴보면 맵리듀스를 이해하는 데 도움이 많이 된다.
* 문서 집합 대상으로 전문 검색이 필요하다면 Batch 처리가 색인을 구축하는데 여전히 효율적이다.
* Mapper는 필요에 따라 문서 집합을 파티셔닝하고 각 Reducer가 해당 파티션에 대한 색인을 구축한다.
#### 일괄 처리의 출력으로 키-값을 저장
* 검색 색인 외에, Machine Learning, 추천시스템을 구축할 수 있다.
* 이런 Batch 처리의 출력은 일종의 데이터베이스가 된다. 
    예를 들어, 사용자에게 추천할 친구를 가져오기 위해 사용자 ID로 질의하는 데이터베이스 또는 관련 상품 목록을 가져오기 위해 상품 ID로 질의하는 데이터베이스가 있다.
* 배치 처리 작업 내부에 완전히 새로운 데이터베이스를 구축해 분산 파일 시스템의 작업 출력 디렉터리에 저장한다.
    * 이 데이터 파일은 한 번 기록되면 불변이고 서버에 벌크로 적재해 읽기전용 질의를 처리할 수 있다.
    * 다양한 키-값 저장소가 맵리듀스 작업내에서 데이터베이스 파일을 구축하는 기능을 지원한다.
        * Voldmort, Terrapin, ElephantDB, Hbas Bulk Loading.

#### Batch 처리 출력에 관한 철학
* 유닉스 도구에서 우리는 아래와 같은 유닉스 철학을 배웠다. 
    * "입력은 변하지 않은 채 새 출력이 이전 출력을 완벽하게 교체한다. 이 과정에서 아무런 부수효과가 없다."
    * 이는 시스템 상태를 엉망으로 만들지 않고도 얼마든지 원하는 만큼 프로그램을 수정하거나 디버깅 용도로 명령을 재실행할 수 있다는 뜻이다.
* 맵리듀스 작업도 마찬가지 철학으로 출력을 취급한다.
    * 입력을 불변으로 처리하고 외부 데이터베이스에 기록하는 등의 부수 효과를 피하기 때문에 배치 처리 작업은 좋은 성능을 내면서도 유지보수가 훨씬 간단하다.
    * 코드에 버그가 있어 출력이 잘못됬거나 오염됐다면 코드를 이전 버전으로 돌리고 작업을 재수행해 간단하게 출력을 고칠 수 있다. (**내결함성**)
    * 쉽게 되돌릴 수 있는 속성의 결과로 실수를 하면 손상을 되돌릴 수 없는 환경에서보다 기능 개발을 빠르게 진행할 수 있다. (**비가역성 최소화**)
### 하둡과 분산 데이터베이스의 비교
* MapReduce 논문이 발간됐을 때, 맵리듀스는 전혀 새로운 개념이 아니였음.
    * 지난 몇 개 절에서 설명했던 처리 알고리즘과 병렬 조인 알고리즘은 수십 년 전에 **대규모 병렬 처리** 데이터베이스라 불리는 것에서 모두 구현됐다.
* MapReduce와의 가장 **큰 차이점**을 보면,
    * MPP 데이터베이스는 장비 클러스터에서 분석 SQL 질의를 병렬로 수행하는 것에 초점
    * MapReduce와 분산 파일 시스템의 조합은 아무 프로그램이나 실행할 수 있는 운영체제와 비슷한 속성을 제공.
#### 저장소의 다양성
* 데이터베이스는 특정 모델(관계형, 문서형)을 따라 데이터를 구조화해야 한다.
    * 반면, 분산 파일 시스템의 파일은 어떤 데이터 모델과 인코딩을 사용해서도 기록할 수 있는 연속된 바이트일 뿐이다. 
    * 이 파일은 데이터베이스 레코드 집합일 수도 있고, 텍스트나 이미지, 비디오, 행렬, 벡터, 게놈 시퀀스 등 다른 어떤 형태도 가능하다.
* Hadoop은 데이터가 어떤 형태라도 상관없이 HDFS로 덤프할 수 있는 가능성을 열어 놓았다.
    * 데이터를 어떻게 처리할지는 덤프 이후에 생각한다.
    * 반대로, MPP 데이터베이스를 사용하면 대게 데이터베이스에 특화된 저장 형태로 데이터를 가져오기 전에 데이터와 질의 형태를 신중하게 모델링해야 한다.
* 어떤사람의 관점에서 보면, 세심하게 모델링하고 데이터를 가져오는 일이 더 바람직하다.
    * 데이터베이스 사용자가 작업하기 좋은 양질의 데이터를 가지게 된다는 뜻이기 때문이다.
    * 그러나 현실에서는 이상적인 데이터 모델을 만들려고 하기보다 데이터를 빨리 사용하게 만드는 것이 더 가치 있다.
* 이 아이디어는 데이터 웨어하우스 개념과 유사하다.
    * 커다란 조직의 다양한 부분에서 나온 데이터를 한 곳(Data Warehouse)에 모으는 작업만으로 큰 가치가 있다.
    * MPP 데이터베이스가 요구하는 세심한 스키마 설계는 중앙 집중식 데이터 수집을 느리게 만든다.
    * Raw 데이터를 수집하고 스키마 설계는 나중에 고민하면 데이터 수집의 속도가 올라간다. ("Data lake", "Enterprise Data Hub")
* 제약없는 데이터 덤핑은 데이터를 해석하는 부담을 이전시킨다.
    * 데이터셋 생산자에게 데이터셋을 표준 형식으로 바꾸게끔 강제하는 대신 데이터 해석은 소비자가 해결할 문제가 된다.
    * 하나의 이상적인 데이터 모델은 존재하지 않을지 몰라도 데이터에는 여러 목적에 적합한 다양한 관점이 존재한다.
    * Raw Data를 덤프하는 것만으로도 이런 변환이 가능하다. 이 접근법은 초밥원리라 부른다. (원시 데이터가 더 좋다.)

#### 처리 모델의 다양성
* MPP데이터베이스는 일체식(Monolithic) 구조로서 디스크 저장소 레이아웃과 질의 계획, 스케줄링과 실행을 다루는 소프트웨어 조각들이 긴밀하게 작동하는 형태.
    * 이 구성 요소들은, 데이터베이스의 특정한 필요에 따라 튜닝하거나 최적화
    * 전체 시스템은 설계된 질의 유형에 대해서는 매우 좋은 성능을 얻을 수 있다.
    * SQL질의 언어를 사용하면 코드를 작성하지 않아도 됨, 
    * Tableau 같은 비즈니스 분석가용 그래픽 도구에 SQL 질의 언어로 접근이 가능.
* 반면 SQL 질의로 모든 종류의 처리를 표현하지는 못함.
    * 머신러닝, 추천 시스템, 전문 검색 색인을 구축, 이미지 분석을 한다면 범용적인 데이터 처리 모델이 필요.
    * 이런 처리는 특정 애플리케이션(기계학습 라이브러리)에 한정되는 경우가 많다.
    * 단순한 SQL 질의 작성이 아닌 코드 작성이 반드시 필요해지게 됨.
* MapReduce를 이용하면 엔지니어는 자신이 작성한 코드(처리 모델)를 대용량 데이터셋 상에서 실행 가능.
* 시간이 흘러, 사람들은 MapReduce가 너무 제한적이고,어떤 점에선 성능도 나쁘다는 점을 깨달음.
    * 그래서 Hadoop위에서 다른 다양한 처리 모델이 개발됐다. 
    * Hadoop 플랫폼의 개방성 때문에 일체식 MPP 데이터베이스에서는 불간으했떤 모든 범위의 접근법을 구현할 수 있게되었다.

#### 빈번하게 발생하는 결함을 줄이는 설계
* MapReduce와 MPP 데이터베이스를 비교할 때 설계방식에서 큰 차이점 두 가지가 두드러진다.
    * 결함을 다루는 방식
    * 메모리 및 디스크를 사용하는 방식.
* MPP데이터베이스에서, 질의 실행 중에 한 장비만 죽어도 전체 질의가 중단된다.
    * 사용자가 질의를 다시 제출하든지, 자동으로 재실행하게 된다.
    * MPP 데이터베이스는 디스크에서 데이터를 읽는 비용을 피하기 위해 
        * 해시 조인 같은 방식을 사용해 가능하면 메모리에 많은 데이터를 유지하는 것을 선호한다.
* MapReduce는 Map, Reduce 태스크의 실패를 견딜 수 있다.
    * 개별 태스크 수준에서 작업을 재수행하기 때문에 전체 작업으로 보면 영향을 받지 않는다.
    * MapReduce는 데이터를 되도록 디스크에 기록하려한다.
    * 내결함성을 확보하기 위함이고, 한편으로는 메모리에 올리기에는 데이터셋이 너무 크다는 가정때문이다.
## 맵리듀스를 넘어
* 2000년대 후반 맵 리듀스는 매우 인기를 끌었고, 그런 인기로 인해 과대 포장된 면이 있었다.
    * 하지만, MapReduce는 분산 시스템에서 여러 모델 중 단지 하나이다.
* 이번 장의 나머지 부분에서는 Batch 처리 방법의 여러 대안을 살펴보는 데 할애한다.

### 중간 상태 구체화 (Materialization of Intermediate State)
* 맵리듀스 작업은 다른 작업과 모두 독립적이다.
    * 첫 번째 작업의 출력을 두 번째 작업의 입력으로 사용하려면,
        * 두 번째 작업의 입력 디렉터리를 첫 번째 작업의 출력 디렉터리와 같게 설정해야 한다.
    * 외부 워크플로 스케줄러에서 반드시 첫번째 작업을 완료한 후에 두 번째 작업을 수행해야 한다.
* 많은 경우 한 작업의 출력은, 다른 특정 작업의 입력으로만 사용된다.
    * 이 경우에 분산 파일 시스템 상에 있는 파일들은 단순히 한 작업에서 다른 작업으로 데이터를 옮기는 수단, **중간상태**(Intermediate state)다.
    * 추천 시스템을 구축할 때 사용하는 복잡한 워크플로는 맵리듀스 작업이 50~100개로 구성되는데 여기에는 많은 중간 상태가 존재한다.
* 중간 상태를 파일로 기록하는 과정을 **구체화**(Materialization)라 한다.
* 중간 상태를 완전히 구체화하는 맵리듀스 접근법은 유닉스 파이프(메모리 버퍼에 저장하는 방법)에 비해 많은 단점이 있다.

#### 데이터플로 엔진
* 맵리듀스에 있는 이런 문제를 해결하기 위해 분산 배치 연산을 수행하는 엔진이 새롭게 개발됐다.
    * 그 가운데 Spark, Tez, Flink가 널리 알려진 엔진이다.
* 설계 방식에서는 서로 많은 차이가 있지만 이 엔진들은 하나 공통점이 있다.
    * 전체 워크플로를 독립된 하위 작업으로 나누지 않고 작업 하나로서 다룬다는 점.
* 이 엔진들은 여러 처리 단계를 통해 데이터 흐름을 명시적으로 모델링하기 때문에 이 시스템을 **데이터플로 엔진**(Dataflow Engine)이라고 부른다.
    * Dataflow engine은 MapReduce처럼 사용자 정의 함수를 반복 호출해 한번에 레코드 1개씩 처리한다.
    * 입력을 파티셔닝해 병렬화한다.
    * 한 함수의 출력을 다른 함수의 입력으로 사용하기 위해 네트워크를 통해 복사한다.
* MapReduce와 달리 이 함수들은 Map과 Reduce를 번갈아 수행하는 식의 규칙을 엄격하게 지킬 필요가 없다.
    * 대신 더 유연한 방법으로 함수들을 조합할 수 있다.
    * 이런 함수를 **연산자**(Operator)라고 부른다.
    * 연산자의 출력과 다른 연산자의 입력을 연결하는 여러 가지 선택지를 제공한다.
* 이런 스타일의 처리 엔진은 MapReduce모델과 비교했을 때 몇 가지 장점이 있다.
    * 정렬과 같은 값 비싼 작업은 실제로 필요할때만 수행한다. (MapReduce는 정렬 작업이 항상 발생.)
    * 연산자간 중간 상태는 대개 메모리나 로컬 디스크에 기록. HDFS에 중간상태를 기록할 때보다 I/O가 훨씬 적게 든다.
    * 연산자들은 입력이 준비되는 즉시 실행을 시작할 수 있다. 다음 단계를 시작하기 위해 선행 단계 전체가 끝나기를 기다릴 필요가 없다.


#### 내결함성
* 분산 파일 시스템에 중간 상태를 모두 구체화할 때 생기는 이점은 내구성이다. 
    * MapReduce는 중간 상태를 모두 구체화하기 때문에 쉽게 내결함성을 확보한다.
* Spark, Flink, Tez는 HDFS에 중간상태를 쓰지 않기 때문에 내결함성 확보를 위해 다른 접근법을 사용한다.
    * 장비가 죽어서 장비에 있던 중간 상태까지 잃게 되면 아직 유효한 데이터로부터 계산을 다시 해서 복구한다.
    * Spark는 데이터 중간상태의 조상을 추적하기 위해 RDD(Resilient distributed dataset)를 사용
    * Flink는 연산자 상태를 체크포인트로 남겨 작업 실행 중 실패한 연산자 수행을 재개할 수 있다.
* 데이터를 재연산할 때 중요한 점은 해당 연산이 결정적인(deterministic)인지 아닌지 파악하는 것이다.
    * 동일한 입력 데이터가 주어졌을 때 연산자들이 항상 같은 출력을 생산?
* 결함에서 복구할 때 데이터를 재연산하는 방식이 항상 정답은 아니다.
    * 중간 데이터가 원천 데이터보다 훨씬 작거나 연산이 CPU중심적이라면, 재연산보다 중간 데이터를 파일로 구체화하는 방식이 더 효과적이다. 
#### 구체화에 대한 논의
* 유닉스에 비유하자면 MapReduce는 각 명령의 출력을 임시 파일에 기록하는 것과 유사.
* DataFlow Engine은 유닉스 파이프와 매우 비슷.
* DataFlow Engine을 사용할 때 HDFS상에 구체화된 데이터셋은 보통 작업의 입력과 출력이다.
* 입력은 불변이고 최종 출력을 완전히 교체하는 방식은 맵리듀스와 비슷하다. 
* 맵리듀스보다 개선된 점은 사용자가 직접 모든 중간 상태를 파일 시스템에 기록하는 수고를 덜어준다는 점이다.

### 고수준 API와 언어
* 이전 설명처럼 직접 맵리듀스 작업을 작성하는 일은 상당히 어렵기 때문에 Hive, Pig, Cascading, Crunch와 같은 고수준 언어나 API가 인기를 끌었다.
* 이런 고수준 인터페이스는 
    * 코드를 적게 작성해도 되고 
    * 대화식 사용도 지원한다.
* 이러한 개발 스타일은 데이터셋을 조사하고 데이터셋을 처리하기 위해 여러 접근법을 실험해 때 매우 유용하다. (유닉스 철학에서, 투명성과 실험)
* 고수준 인터페이스를 사용하면 사용자가 시스템을 생산성 높게 사용할 수 있을 뿐 아니라 장비 수준에서도 작업을 더욱 효율적으로 수행할 수 있다.
#### 선언형 질의 언어로 전환
* 조인을 수행하는 코드를 작성하는 방식 대신에, 관계형 연산자로 조인을 나타내면 프레임워크가 조인 입력의 속성을 분석해 자동으로 앞서 기술한 조인 알고리즘 중에 어떤 방법이 적절한지 자동으로 결정할 수 있다. (*Hive*, *Spark*, *Flink*는 이런식으로 작동하는 비용 기반의 질의 최적화기를 내적하고 있다.)
* 어떤 조인 알고리즘을 선택하느냐에 따라 배치 작업의 성능이 크게 달라진다.
    * 이번 장에서 설명한 모든 다양한 조인 알고리즘을 모두 이해하고 기억할 필요는 없다.
    * **선언적인** 방법으로 조인을 지정하면 질의 최적화기가 최적의 수행방법을 결정한다.
* 어떤 면에서는 맵리듀스와 맵리듀스의 데이터플로 계승자들은 SQL의 선언형 질의 모델과는 많이 다르다.
    * 맵리듀스의 함수에는 출력을 결정하는 코드를 **사용자가 임의로 작성**할 수 있다.
        * 이 때문에, 파싱, 자연어 분석, 이미지 분석, 수치 또는 통계 알고리즘을 수행할 수 있게 된다.
        * 코드를 임의로 실행할 수 있다는 점은 MPP 데이터베이스와 맵리듀스를 계승하는 배치 프로 시스템을 오랜 세월 동안 구별해준 특성이다.
* 때로는, 데이터플로 엔진도 조인 외에 좀 더 선언적인 기능을 통합하면 이점이 있다. (Filtering 작업)
* 고수준 API에 선언적 측면을 포함하면서 실행 중에 이용할 수 있는 질의 최적화기를 가진다면 배치 처리 프레임워크는 MPP 데이터베이스와 한층 비슷해진다. 동시에 배치 처리 프레임워크는 임의의 코드를 실행하고 임의 형식의 데이터를 읽을 수 잇는 확장성을 지녀, 배치 처리 프레임워크의 장점인 유연성은 그대로 유지한다.

## 정리
* 이번 장에서는 배치 처리에 대해 배웠다.
* `awk`, `grep`, `sort` 등의 유닉스 도구를 살펴보는 것을 시작으로 이 도구들의 설계 철학이 어떻게 맵리듀스와 최근에 개발된 데이터플로 엔진에 녹아 있는지 살펴봤다.
* 이 설계 원리 중 몇가지를 뽑아보면 다음과 같다.
    * 입력은 불변이고 출력은 다른 프로그램의 입력으로 사용한다.
    * 복잡한 문제도 "한 가지 일을 잘하는" 작은 도구를 엮어서 해결한다.
* 유닉스 환경에서 프로그램과 다른 프로그램을 연결하는 단일 인터페이스는 파일과 파이프이다.
    * 맵리듀스의 인터페이스는 분산 파일 시스템이다.
    * 데이터플로 엔진은 파이프와 비슷한 자체 데이터 전송 메커니즘을 사용해 분산 파일 시스템에 중간 상태를 구체화하는 것을 피한다.
    * 다만 작업의 초기 입력과 최종 출력은 여전히 HDFS를 사용한다.
* 앞서 맵리듀스에서 사용하는 몇 가지 조인 알고리즘에 대해 알아봤다. 
    * **정렬 병합 조인**
        * 조인할 각 입력은 조인 키를 추출하는 Mapper를 통과. 파티셔닝, 정렬, 병합 과정을 마치면 같은 키를 가지는 모든 레코드는 하나의 리듀서에서 호출된다. 이 리듀서 함수에서 병합된 레코드를 출력.
    * **브로드캐스트 해시 조인**
        * 조인할 입력 두 개 중 하나가 상대적으로 적다면 파티셔닝하지 않고 해시 테이블에 모두 적재할 수 있다. 큰 조인 입력의 각 파티션에서 매퍼를 시작할 때 각 매퍼에 작은 입력으로 만들어진 해시테이블을 적재하고 큰 입력에서 한 번에 하나씩 레코드를 스캔하면서 각 레코드가 해시 테이블에 조냊하는지 질의.
    * **파티션 해시 조인**
        * 조인 입력 두 개를 같은 방식으로 파티셔닝하면 해시 테이블 바식을 각 파티션별로 독립적으로 사용할 수 있다.
* 분산 배치 처리 엔진은 의도적으로 제한된 프로그래밍 모델을 제공.
    * Mapper와 Reducer같은 콜백함수는 상태 정보가 없다고 가정.
    * 지정된 출력 외에 외부에서 보이는 부수 효과가 없어야 한다.
    * 이 제한으로 프레임워크는 분산 시스템에서 발생하는 어려운 문제들을 추상화 아래로 숨길 수 있다.
* 배치 처리 작업의 차별화된 특징은 입력을 수정하지 않고 입력을 읽어 출력을 생산한다는 점.
    * 다시 말하자면 출력은 입력으로부터 파생.
    * 결정적으로 입력 데이터는 **고정된 크기로 한정**.
    * 크기가 한정되기 때문에 작업은 입력 전체를 다 읽었는지 알 수 있고, 결과적으로 작업을 종료할 수 있다.
* 다음장에서는 스트림 처리를 다룬다.
    * 스트림 처리에서는 입력이 **한정되지 않는다**. 즉 작업의 입력은 끝이 없는 데이터 스트림이다. 이 경우 작업은 결코 끝나지 않는다. 언제라도 입력이 더 들어올 수 있기 때문이다.
    